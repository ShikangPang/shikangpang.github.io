<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>SkPang</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Shikang Pang&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Shikang Pang&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Artificial Intelligence Deep Learning Computer Science"><meta property="og:type" content="website"><meta property="og:title" content="SkPang"><meta property="og:url" content="http://shikangpang.github.io/"><meta property="og:site_name" content="SkPang"><meta property="og:description" content="Artificial Intelligence Deep Learning Computer Science"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://shikangpang.github.io/img/og_image.png"><meta property="article:author" content="Shikang Pang"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://shikangpang.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://shikangpang.github.io"},"headline":"SkPang","image":["http://shikangpang.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Shikang Pang"},"publisher":{"@type":"Organization","name":"SkPang","logo":{"@type":"ImageObject","url":"http://shikangpang.github.io/img/logo.svg"}},"description":""}</script><link rel="alternate" href="https://shikangpang.github.io/" title="SkPang" type="application/atom+xml"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="SkPang" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Shikang&#039;s Blog</a><a class="navbar-item" href="/curriculum/">CURRICULUM</a><a class="navbar-item" href="/blog/">BLOG</a><a class="navbar-item" href="/articles/">ARTICLES</a><a class="navbar-item" href="/projects/">PROJECTS</a><a class="navbar-item" href="/publications/">PUBLICATIONS</a><a class="navbar-item" href="/readings/">READINGS</a><a class="navbar-item" href="/life/">LIFE</a><a class="navbar-item" href="/essays/">ESSAYS</a><a class="navbar-item" href="/archives/">Archives</a><a class="navbar-item" href="/categories/">Categories</a><a class="navbar-item" href="/tags/">Tags</a><a class="navbar-item" href="/study/">STUDY</a><a class="navbar-item" href="/faqs/">FAQs</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="My GitHub Url" href="https://github.com/shikangpang"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/blog/Sobel%20Operator/">Sobel Operator</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-10-16</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-16</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i>a few seconds read (About 0 words)</span></div></div><div class="content"></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/life/life/">life</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-10-16</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-16</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/life/">life</a></span><span class="level-item"><i class="far fa-clock"></i>a few seconds read (About 0 words)</span></div></div><div class="content"></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/detection/">detection</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-10-16</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-16</span><span class="level-item"><i class="far fa-clock"></i>a few seconds read (About 0 words)</span></div></div><div class="content"></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/readings/%E7%99%BE%E5%B9%B4%E5%AD%A4%E7%8B%AC/">百年孤独</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-10-16</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-16</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/readings/">readings</a></span><span class="level-item"><i class="far fa-clock"></i>a few seconds read (About 2 words)</span></div></div><div class="content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/study/normal_psy/">普通心理学</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-10-16</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-11-25</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/study/">study</a></span><span class="level-item"><i class="far fa-clock"></i>18 minutes read (About 2641 words)</span></div></div><div class="content"><h2 id="第一章-心理学的研究对象和方法"><a href="#第一章-心理学的研究对象和方法" class="headerlink" title="第一章 心理学的研究对象和方法"></a>第一章 心理学的研究对象和方法</h2><h3 id="一、心理学的研究对象"><a href="#一、心理学的研究对象" class="headerlink" title="一、心理学的研究对象"></a>一、心理学的研究对象</h3><p>研究对象：人的心理现象<br>一、个体心理<br>（一）认知<br>感觉、知觉、意识和注意、记忆、思维、语言<br>（二）动机和情绪<br>（三）能力和人格<br>二、行为<br>三、意识<br>四、社会心理</p>
<h3 id="二、心理学的任务"><a href="#二、心理学的任务" class="headerlink" title="二、心理学的任务"></a>二、心理学的任务</h3><h3 id="三、心理学的研究方法"><a href="#三、心理学的研究方法" class="headerlink" title="三、心理学的研究方法"></a>三、心理学的研究方法</h3><p>观察法<br>测验法<br>相关法<br>实验法<br>个案法</p>
<h3 id="四、心理学的过去和现在"><a href="#四、心理学的过去和现在" class="headerlink" title="四、心理学的过去和现在"></a>四、心理学的过去和现在</h3><h4 id="构造主义"><a href="#构造主义" class="headerlink" title="构造主义"></a>构造主义</h4><p>主要人物：冯特、铁钦纳</p>
<p>研究对象：直接经验</p>
<p>研究方法：实验内省法</p>
<p>主要观点：主张心理学的研究对象是人的意识即对直接经验的觉知。分析意识的内容，并找出意识的主要成分，以及如何联结组成复杂心理过程的规律。从意识经验的够早来说明整个人的心理，只看重意识的成分，而不管意识内容的来源，意义和作用。认为够成啊人的心理世界的基本成分：感觉(知觉的元素),表象(观念的元素),意识(情绪的元素)。</p>
<p>贡献：使心理学摆脱了哲学的束缚，开创了现代心理学并为其发展奠定了基础。</p>
<p>局限：① 研究内容狭窄、脱离实际;②把心里简单分解为各个元素，割裂其整体性；③过于主观，可重复性差,许多高级心理现象难以通过内省进行研究。</p>
<h4 id="机能主义"><a href="#机能主义" class="headerlink" title="机能主义"></a>机能主义</h4><p>主要人物：詹姆斯、杜威、安吉尔</p>
<p>研究对象：意识的作用和功能</p>
<p>研究方法：客观观察法、实验内省法</p>
<p>主要观点：主张心理学的研究对象是具有适应性的心理活动，强调意识活动在人类有机体的需要与环境之间起重要的中介作用，把意识看成是川流不息的过程。机能主义的特点：① 反对把意识分析为感觉、感情等元素，主张意识是一个连续的整体。②反对把心理视为一种不起作用的副现象，强调心理的适应功能。③反对把心理学只看作一门纯科学，重视心理学的实际应用。④反对把心理学局限于对正常人一般心理规律的探索，主张把心理学的研究范围扩大到动物心理、儿童心理、教育心理、变态心理等领域。</p>
<p>贡献：① 开创了美国的科学心理学；②促进了心理学分支学科的发展；③推动了心理学的广泛应用。</p>
<p>局限：① 意识观的矛盾倾向；②生物主义的倾向；③外在目的论和神秘主义倾向。</p>
<h4 id="行为主义"><a href="#行为主义" class="headerlink" title="行为主义"></a>行为主义</h4><p>主要人物：华生、斯金纳、班杜拉</p>
<p>研究对象：行为</p>
<p>研究方法：实验法</p>
<p>主要观点：①反对研究意识，主张心理学应当研究行为；②反对内省，主张使用实验方法；③反对行为的遗传决定论，强调环境的作用。行为主义认为：心理、意识和灵魂是主观的东西，不可捉摸，又不能加以观察、测量和证实，作为研究对象，永远不可能跻身科学之列。心理学家需要研究的只有那些可以被观察、预见、最终可以被科学工作者控制的行为。</p>
<p>贡献：行为主义的诞生，在世界各国的心理学界产生了很大的反响。其强调的用客观方法研究可以观察的行为，对心理学走上科学的道路有积极的作用。<br>局限：主张过于极端，不研究心理的内部结构和过程，否定研究意识的重要性，限制了心理学的健康发展。</p>
<h4 id="格式塔心理"><a href="#格式塔心理" class="headerlink" title="格式塔心理"></a>格式塔心理</h4><p>主要人物：韦特海默、柯勒、考夫卡</p>
<p>研究对象：意识、认知现象（知觉、学习、思维等）</p>
<p>研究方法：实验法</p>
<p>主要观点：反对把意识作为一个元素，而强调心里作为一个整体、一种组织的意义。”格式塔“是德文”完整“的译音，每一种心理现象都是一个格式塔。整体不是由若干元素组合而成的，相反整体先于部分而存在并且制约着部分的性质和意义，整体大于部分的综合。他们提出了知觉中的许多组织原则，试图解决格式塔的生理基础问题。他们坚决反对对任何心理现象进行元素的分析，并把冯特的构造心理学称为”砖块和灰泥的心理学“</p>
<p>历史评价：格式塔心理强调整体并不等于部分的简单加和，整体先于部分而存在并制约着部分的性质和意义的理论观点，是正确的。此外，格式塔心理学家关于只觉得组织原则及学习和思维中的研究成果至今仍反映在心理学教科书中。</p>
<h4 id="精神分析"><a href="#精神分析" class="headerlink" title="精神分析"></a>精神分析</h4><p>主要人物：弗洛伊德、荣格、阿德勒</p>
<p>研究对象：梦、潜意识、无意识</p>
<p>研究方法：梦的解析、催眠疗法、自由联想、生活史法</p>
<p>主要观点：人类的一切个体和社会行为，都根源于心灵深处的某种欲望或动机，特别是性欲的冲动。欲望以无意识的形式支配人，并且表现在人的正常和异常行为中。其理论主要来源于精神病治疗的临床实践。</p>
<p>贡献：弗洛伊德对精神病学和临床心理学的影响是深远的，对科学心理学也有相当重要的意义。弗洛伊德理论中的一些概念，如潜意识动机、防御机制等已被主流心理学所采纳。不仅对心理学，甚至对人类文化都产生了极其深远的影响。弗洛伊德《梦的解析》与达尔文的《物种起源》、哥白尼的《天体运动论》并称为导致人类三大思想革命的经典之作。</p>
<p>局限：研究方法缺乏科学的严谨性、过度强调无意识、并与意识对立起来，夸大性欲的作用一直受到科学心理学家的批评。</p>
<h4 id="人本主义"><a href="#人本主义" class="headerlink" title="人本主义"></a>人本主义</h4><p>主要人物：马斯洛、罗杰斯</p>
<p>研究对象：人格</p>
<p>研究方法：整体分析、个案研究</p>
<p>主要观点：①认为人性本善，强调人性的显著特点是”持续不断地成长“，认为人性是自主的，是能进行自我选择的，把人当作一个完整的人来看待。<br>②强调”以人为本“:尊重和重视个体的基本需要、尊重和重视自我。关注人的价值，强调人有自我实现的需要。③需要理论：需要是有机体内部的某种缺乏或不平衡状态，表现为有机体的生存和发展对客观条件的依赖性，是有机体活动的积极源泉。马斯洛提出了需要层次理论。</p>
<p>贡献：人本主义被称为除行为主义和精神分析之外，心理学上的”第三势力“。其主张人性本善，注重对人性的研究，具有人文关怀的思想。对传统心理学的某些批判，具有一定的启发作用。人本主义的研究理念冲淡了心理学的纯科学色彩。</p>
<p>局限：人本主义错误地理解人的本质，把人看成人性的人，而不是社会关系的综合，因而对人内心世界的某些描述常常是从个人出发的。此外，人本主义心理学的许多主张带有纲领的性质，所使用的名词缺乏明确的定义，也没有具体说明所采用的研究方法，使得其理论难以得到检验。</p>
<h4 id="认知心理学"><a href="#认知心理学" class="headerlink" title="认知心理学"></a>认知心理学</h4><p>主要人物：皮亚杰、奈塞尔、纽维尔、西蒙</p>
<p>研究对象：研究人的高级心理过程，主要是认知过程，如注意、知觉、表象、记忆、思维和言语等。</p>
<p>研究方法：反应时记录法、口语报告法、计算机模拟</p>
<p>主要观点：①关注人脑所发生的心理事件，如人是怎样推理、记忆、理解语言、解决问题、解释经验、获得道德标准和形成信念的。②主体在学习中不是机械地接受刺激，被动地做出反应，而是主动地有选择地获取刺激并进行加工。③使用信息加工的观点和术语来解释人的认知过程，即把人脑比做电脑。④认为认知历程包括：信息的接收、贮存和运用。</p>
<p>历史评价：认知心理学在20世纪50年代以后得到迅速发展。近年来，认知心理学与神经科学的结合产生了认知神经科学。科学家们认为，只有揭示心理活动的脑机制，特别是认知功能的神经生物学机制，才能真正揭示脑的秘密，了解人的心里功能的特点。认知心理学在心理学中占有的地位越来越重要。</p>
<h2 id="第二章-神经生理机制"><a href="#第二章-神经生理机制" class="headerlink" title="第二章 神经生理机制"></a>第二章 神经生理机制</h2><h3 id="韦伯定理"><a href="#韦伯定理" class="headerlink" title="韦伯定理"></a>韦伯定理</h3><h3 id=""><a href="#" class="headerlink" title=""></a></h3></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/">About Me</a></p><div class="content"><h1 id="Shikang-Pang"><a href="#Shikang-Pang" class="headerlink" title="Shikang Pang"></a>Shikang Pang</h1><h1 id="Who-am-I"><a href="#Who-am-I" class="headerlink" title="Who am I"></a>Who am I</h1><p>Hello, my name is Shikang Pang, a master’s graduate in Computer Science and Technology from Henan University. Currently, I am a Software Development Engineer at Xiaomi, focusing on applying large language models to code assistance. I am passionate about solving complex problems through technology and have extensive experience in full-stack development and large language models. During my studies, I specialized in graph anomaly detection, and I have also co-founded two startups, gaining valuable project and management experience.</p>
<hr>
<h1 id="About-This-Site"><a href="#About-This-Site" class="headerlink" title="About This Site"></a>About This Site</h1><p>This site is built with <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> and <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo-theme-icarus">Icarus</a>. The theme is based on the <a target="_blank" rel="noopener" href="https://bulma.io/">Bulma</a> framework. The site is hosted on <a target="_blank" rel="noopener" href="https://pages.github.com/">GitHub Pages</a>.</p>
<p>The main purpose of this site is to share my thoughts and experiences with the world. I hope that it will be a valuable resource for anyone who is interested in learning more about artificial intelligence, large language models, and their applications.</p>
<hr>
<h1 id="Citations"><a href="#Citations" class="headerlink" title="Citations"></a>Citations</h1><p>If you would like to cite the blog posts and articles in this site or repositories on my <a target="_blank" rel="noopener" href="https://github.com/shikangpang">Github</a>, please use the URLs. I will not change the URLs, unless someday GitHub does not exist.</p>
</div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/articles/OpenAI%20GPT%20Models/">OpenAI GPT Models</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-03-11</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-17</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/articles/">articles</a></span><span class="level-item"><i class="far fa-clock"></i>a minute read (About 199 words)</span></div></div><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>OpenAI Generative Pre-trained Transformer (GPT) models are a series of transformer</div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><span class="mr-2"><i class="fas fa-tags has-text-grey"></i></span><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Inference-Optimization/">Inference Optimization</a><a class="link-muted mr-2" rel="tag" href="/tags/Transformer/">Transformer</a><a class="link-muted mr-2" rel="tag" href="/tags/GPT/">GPT</a><a class="link-muted mr-2" rel="tag" href="/tags/OpenAI/">OpenAI</a><a class="link-muted mr-2" rel="tag" href="/tags/ChatGPT/">ChatGPT</a><a class="link-muted mr-2" rel="tag" href="/tags/ACCELERATED-COMPUTING/">ACCELERATED COMPUTING</a></div><a class="article-more button is-small is-size-7" href="/articles/OpenAI%20GPT%20Models/#more"><i class="fas fa-book-reader has-text-grey"></i>  Read more</a></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/articles/%E6%B5%85%E8%B0%88LLM/">浅谈LLM</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2024-03-11</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-18</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/articles/">articles</a></span><span class="level-item"><i class="far fa-clock"></i>35 minutes read (About 5214 words)</span></div></div><div class="content"><h1 id="LLM是什么"><a href="#LLM是什么" class="headerlink" title="LLM是什么"></a>LLM是什么</h1><p>指的是包含超大规模参数（通常在十亿个以上）的神经网络模型，这些模型在自然语言处理领域得到了广泛应用。 大模型具有以下显著特征： 巨大的规模：大模型包含数十亿个参数，模型大小可以达到数百GB甚至更大。 这种巨大的模型规模使它们拥有强大的表达能力和学习能力。<strong>数据量大，参数量大，算力大。</strong><br><strong>兴起的主要原因</strong><br>研究人员发现大模型在40B以上时会有涌现现象</p>
<ul>
<li>数学基础</li>
<li>心理学基础</li>
<li>具体步骤与内容</li>
<li>遇到什么问题</li>
<li>解决方案</li>
</ul>
<h1 id="大模型的数学基础"><a href="#大模型的数学基础" class="headerlink" title="大模型的数学基础"></a>大模型的数学基础</h1><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><p><strong><code>模型参数训练部分</code></strong><br>矩阵乘法<br>加权求和</p>
<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><p><strong><code>损失部分</code></strong><br>准确率<br>贝叶斯估计<br>马尔可夫链<br>交叉熵损失</p>
<h1 id="大模型的心理学基础"><a href="#大模型的心理学基础" class="headerlink" title="大模型的心理学基础"></a>大模型的心理学基础</h1><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p>深度学习—经典条件作用说<br>巴普洛夫的狗<br>把数据和标签提供给神经网络。神经网络对网络进行更新训练以使得神经网络中的结果和标签一致<br>强化学习—操作性条件作用说<br>斯金纳箱<br>把数据提供给神经网络。并在阶段中不断给予惩罚与奖励。促使网络向着有利的方向进行进化（了解不深）</p>
<h2 id="记忆"><a href="#记忆" class="headerlink" title="记忆"></a>记忆</h2><p>大模型中的上下文信息<br>遗忘<br>识记过的信息不能回忆也不能再认，或者发生错误的回忆或再认。</p>
<h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><h3 id="归纳推理"><a href="#归纳推理" class="headerlink" title="归纳推理"></a>归纳推理</h3><p>从特殊性归纳出普通性，类似于概念形成。（模型训练）</p>
<h3 id="演绎推理"><a href="#演绎推理" class="headerlink" title="演绎推理"></a>演绎推理</h3><p>从普通性推理出特殊性，类似于问题解决。（模型预测）</p>
<h2 id="智力"><a href="#智力" class="headerlink" title="智力"></a>智力</h2><p><img src="/images/article/LLM-zhilitu.jpeg" alt="图片"></p>
<h1 id="训练大模型需要哪些内容"><a href="#训练大模型需要哪些内容" class="headerlink" title="训练大模型需要哪些内容"></a>训练大模型需要哪些内容</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="Transformer架构（注意力机制）"><a href="#Transformer架构（注意力机制）" class="headerlink" title="Transformer架构（注意力机制）"></a>Transformer架构（注意力机制）</h3><p><img src="/images/article/Transformer.png" alt="图片"><br>它的结构包括两部分：Encoder（编码器）和Decoder（解码器）。Encoder 与 Decoder 大致都包含以下层，每一层都有特定的功能，下面为 Encoder（编码器）各层的简单介绍：</p>
<ul>
<li>输入嵌入层（Input Embedding Layer）：将输入文本的词或标记转换为向量表示，以便模型能够理解它们。</li>
<li>位置编码（Positional Encoding）：句子中词语相对位置的编码，保留词语的位置信息。</li>
<li>多头自注意力层（Multi-Head Self-Attention Layer）：帮助模型捕捉输入序列中词与词之间的关系，使模型能够了解上下文信息。</li>
<li>前馈神经网络层（Feed-Forward Neural Network Layer）：对多头自注意力的输出进行进一步的特征提取和变换，以增加模型的表示能力。</li>
<li>归一化层（Layer Normalization Layer）：规范化每一层的输出，有助于训练过程的稳定性。</li>
<li>总的来说，Transformer 是一种强大的模型，它可以捕捉文本和序列数据中的长距离依赖关系，使其在翻译、对话、摘要生成等自然语言处理任务中表现出色。这个模型已经在各种应用中取得了显著的成功。<br>感兴趣的同学可以自行去网上搜索下 Transformer 的结构，深入了解。<br><img src="/images/article/Evalution.png" alt="图片"></li>
</ul>
<h2 id="LLAMA的结构"><a href="#LLAMA的结构" class="headerlink" title="LLAMA的结构"></a>LLAMA的结构</h2><p><img src="/images/article/Llana.png" alt="图片"><br>我们来解读下上面 Llama 各层的结构与作用，首先从输入文本开始。会经过下面各层：</p>
<ul>
<li>Input Embedding：将 Input 文本转化为向量表，通过 nn.Embedding 实现。</li>
<li>Llama Decoder Layer：Decoder 采用多层 Llama Decoder Layer。每一层包括自注意力（Llama Attention）和前馈网络（Llama MLP）。自注意力用于捕捉文本中的长程依赖关系。前馈网络进行非线性映射。</li>
<li>Llama RMSNorm：一种规范化方式，用于正则化每层的输，起到预处理的作用。</li>
<li>lm_head：一个线性层，将 Decoder 最后一层的输出映射到词典大小的维，以进行后续的语言模型 Logits 计算。</li>
<li>Llama Attention：多头自注意力机制，用于建模文本中的依赖关系。将输入表示切分为多个头，然后在每个头内做点积注意力运算。</li>
<li>Llama MLP：采用 Gated Linear Units 的多层前馈网络。进行非线性变换来捕捉复杂模式。<br>数据集（信息）<center><b>GPT-3 dataset 为499B tokens</b></center></li>
</ul>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Quantity</th>
<th>Weight in Training Mix</th>
<th>Epochs Elapsed when Training for 300B Tokens</th>
</tr>
</thead>
<tbody><tr>
<td>Common Crawl</td>
<td>410B</td>
<td>60%</td>
<td>0.44</td>
</tr>
<tr>
<td>WebText2</td>
<td>19B</td>
<td>22%</td>
<td>2.9</td>
</tr>
<tr>
<td>Books1</td>
<td>12B</td>
<td>8%</td>
<td>1.9</td>
</tr>
<tr>
<td>Books2</td>
<td>55B</td>
<td>8%</td>
<td>0.43</td>
</tr>
<tr>
<td>Wikipedia</td>
<td>3B</td>
<td>3%</td>
<td>3.4</td>
</tr>
</tbody></table>
<center><b>Llama1 dataset为1.4T tokens</b></center>

<table>
<thead>
<tr>
<th>Dataset</th>
<th>Sampling prop.</th>
<th>Epochs</th>
<th>Disk size</th>
</tr>
</thead>
<tbody><tr>
<td>Common Crawl</td>
<td>67%</td>
<td>1.10</td>
<td>3.3TB</td>
</tr>
<tr>
<td>C4</td>
<td>15%</td>
<td>1.06</td>
<td>783GB</td>
</tr>
<tr>
<td>Github</td>
<td>4.5%</td>
<td>0.64</td>
<td>328GB</td>
</tr>
<tr>
<td>Wikipedia</td>
<td>4.5%</td>
<td>2.45</td>
<td>83GB</td>
</tr>
<tr>
<td>Books</td>
<td>4.5%</td>
<td>2.23</td>
<td>85GB</td>
</tr>
<tr>
<td>ArXiv</td>
<td>2.5%</td>
<td>1.06</td>
<td>92GB</td>
</tr>
<tr>
<td>StackExchange</td>
<td>2.0%</td>
<td>1.03</td>
<td>78GB</td>
</tr>
</tbody></table>
<p><strong>Llama3 dataset为15T tokens</strong></p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><blockquote>
<p>1B参数的大模型的训练需要64块A100显卡<br>GPT3的175B模型训练使用了1024块A100显卡共训练34天<br>Llama 3 405B 使用了16K H100 显卡<br>Epochs 一般为10个epoch以内</p>
</blockquote>
<p><img src="/images/article/Train-loss.jpeg" alt="图片"></p>
<h2 id="持续预训练"><a href="#持续预训练" class="headerlink" title="持续预训练"></a>持续预训练</h2><p>在原有数据基础上，继续进行预训练，数据量和预训练相同<br>对齐<br>在LLM对齐问题上，OpenAI提出的RLHF训练范式（人类反馈强化学习）最为人熟知，同时也是ChatGPT行之有效的对齐方案。<br>RLHF通常包含三个步骤：SFT, Reward Model, PPO, 该方案优点不需多说，缺点也很明显：训练流程繁琐、算法复杂、超参数多和计算量大，因此RLHF替代方案层出不穷。</p>
<h1 id="大模型会遇到哪些问题"><a href="#大模型会遇到哪些问题" class="headerlink" title="大模型会遇到哪些问题"></a>大模型会遇到哪些问题</h1><table>
<thead>
<tr>
<th>问题</th>
<th>灾难性遗忘</th>
<th>幻觉</th>
</tr>
</thead>
<tbody><tr>
<td>定义</td>
<td>新任务学习时忘记旧任务</td>
<td>生成虚假或不准确的信息</td>
</tr>
<tr>
<td>表现</td>
<td>旧任务性能下降</td>
<td>生成的内容看似合理但不真实</td>
</tr>
<tr>
<td>发生场景</td>
<td>增量学习，持续学习</td>
<td>生成式模型如NLP中的文本生成</td>
</tr>
<tr>
<td>主要原因</td>
<td>参数更新过度适应新任务</td>
<td>生成概率最高的序列与实际情况不符</td>
</tr>
<tr>
<td>解决方法</td>
<td>知识蒸馏，正则化技术，增量学习，模型集成</td>
<td>增强数据多样性，使用外部知识库，严格的约束条件</td>
</tr>
</tbody></table>
<h2 id="Agent智能体"><a href="#Agent智能体" class="headerlink" title="Agent智能体"></a>Agent智能体</h2><p>吴恩达提出：</p>
<ul>
<li>反思（Reflection）：Agent通过交互学习和反思来优化决策。</li>
<li>工具使用（Tool use）：Agent 在这个模式下能调用多种工具来完成任务</li>
<li>规划（Planning）：在规划模式中，Agent 需要规划出一系列行动步骤来达到目标。</li>
<li>多Agent协作（Multiagent collaboration）：涉及多个Agent之间的协作。<br>Liliang Weng提出</li>
<li>规划：Agent需要具备规划（同时也包含决策）能力，以有效地执行复杂任务。这涉及子目标的分解（Subgoal decomposition）、连续的思考（即思维链，Chain of thoughts）、自我反思和批评（Self-critics），以及对过去行动的反思（Reflection）。</li>
<li>记忆: 包含了短期记忆和长期记忆两部分。短期记忆与上下文学习有关，属于提示工程的一部分，而长期记忆涉及信息的长时间保留和检索，通常是通过利用外部向量存储和快速检索。</li>
<li>工具：这包括了Agent可能调用的各种工具，如日历、计算器、代码解释器和搜索功能，以及其他可能的工具。由于大模型一旦完成预训练，其内部能力和知识边界基本固定下来，而且难以拓展，那么这些工具显得异常重要。它们扩展了Agent的能力，使其能够执行超出其核心功能的任务。</li>
<li>执行（或称行动）：Agent基于规划和记忆来执行具体的行动。这可能包括与外部世界互动，或者通过工具的调用来完成一个动作（任务）。</li>
</ul>
<h2 id="CoT思维链"><a href="#CoT思维链" class="headerlink" title="CoＴ思维链"></a>CoＴ思维链</h2><p>Chain-of-Thought(CoT)是一种改进的Prompt技术，目的在于提升大模型LLMs在复杂推理任务上的表现，对于复杂问题尤其是复杂的数学题大模型很难直接给出正确答案。如算术推理（arithmetic reasoning）、常识推理（commonsense reasoning）、符号推理（symbolic reasoning）。COT通过要求模型在输出最终答案之前，显式输出中间逐步的推理步骤这一方法来增强大模型的算数、常识和推理能力。简单，但有效。</p>
<h3 id="ReAct框架（Reasoning-and-Acting）"><a href="#ReAct框架（Reasoning-and-Acting）" class="headerlink" title="ReAct框架（Reasoning-and-Acting）"></a>ReAct框架（Reasoning-and-Acting）</h3><p>这个框架整合了先前的CoT和Reflection方法，并引入了工具调用功能，进一步增强了模型的交互能力和应用范围，代表了在Agent认知框架发展中的一个新的里程碑。<br><img src="/images/article/cot.png" alt="图片"><br><strong>ReAct论文中指出，既要有推理，又要有行动</strong><br>ReAct框架是推理和行动的整合，Reasoning and Acting, ReAct框架的核心思想在于在思考，观察和行动 反复循环，迭代，不断优化解决方案，知道问题最终解决位置，这就不仅使Agent能够进行复杂的内部推理，还能实时反应并调整其行为以适应不断变化的环境和需求。<br><img src="/images/article/React.png" alt="图片"><br>目前，ReAct框架已经被无缝集成至LangChain，开发者可以非常轻松地创建ReAct Agent来完成具体任务。</p>
<h3 id="计划与执行框架（Plan-and-Execute）"><a href="#计划与执行框架（Plan-and-Execute）" class="headerlink" title="计划与执行框架（Plan-and-Execute）"></a>计划与执行框架（Plan-and-Execute）</h3><p>Plan-and-Execute可以翻译为计划与执行架构。这种架构侧重于先规划一系列的行动，然后执行。它使LLM能够先综合考虑任务的多个方面，然后按计划行动。在复杂的项目管理或需要多步骤决策的场景中尤为有效，如自动化工作流程管理。<br><img src="/images/article/plan-and-execute.png" alt="图片"><br>Plan-and-Solve论文中的示例<br>目前，LangChain的Experiment（实验包）中支持Plan-and-Execute框架，开发者可以尝试创建Plan-and-Execute Agent，对任务先计划，再具体执行。<br><img src="/images/article/plan-and-solve.png" alt="图片"><br>Plan-and-Solve的实现示例</p>
<h3 id="多Agent协作（Multi-Agents-Collaboration）"><a href="#多Agent协作（Multi-Agents-Collaboration）" class="headerlink" title="多Agent协作（Multi-Agents Collaboration）"></a>多Agent协作（Multi-Agents Collaboration）</h3><p>多Agent系统（Multi-Agent System）的确是一个新的研究热点。这类研究关注如何使多个Agent协同工作，实现复杂的任务和目标。这包括合作、竞争以及协商策略的研究。<br>这类多Agent协作框架的代表性作品是AutoGen和MetaGPT。<br>AutoGen框架中的Agent定制（Agent Customization）功能允许开发者对Agent进行定制，用以实现不同的功能。<br>MetaGPT的框架，它将标准操作程序（SOPs）与基于大模型的多智能体系统相结合，使用SOPs来编码提示，确保协调结构化和模块化输出。这种框架允许智能体在类似流水线方法的范式中扮演多样化的角色，通过结构化的智能体协作和强化领域特定专业知识来处理复杂任务，提高在协作软件工程任务中解决方案的连贯性和正确性。<br>MetaGPT的Demo中，构建了一个软件公司场景下的多Agent软件实体，它能够处理复杂的任务，模仿软件公司的不同角色。其核心理念是”代码等同于团队的标准操作程序（Code &#x3D; SOP(Team)）”，将标准操作程序具体化并应用于由大模型组成的团队。<br><img src="/images/article/Multi-Agent.png" alt="图片"><br>软件公司组织角色图<br>这个软件公司的组织角色图突出了公司内的不同角色及其职责。</p>
<p>老板（Boss）：为项目设定总体要求。<br>产品经理（Product Manager）：负责编写和修订产品需求文档（PRD）。<br>架构师（Architect）：编写和修订设计，审查产品需求文档和代码。<br>项目经理（Project Manager）：编写任务，分配任务，并审查产品需求文档、设计和代码。<br>工程师（Engineer）：编写、审查和调试代码。<br>质量保证（QA）：编写和运行测试，以确保软件的质量。<br>你只要输入一行具体的软件开发需求。经过几轮协作，MetaGPT的假想软件工程团队就能够开发出真正可用的简单APP。</p>
<p>当然MetaGPT的功能不仅限于此，还可以用于其他场景构建应用程序。</p>
<h3 id="各种认知框架的组合运用"><a href="#各种认知框架的组合运用" class="headerlink" title="各种认知框架的组合运用"></a>各种认知框架的组合运用</h3><p>上述认知框架当然是可以的组合的比如说，ReAct框架中，就一定应该配置Tool Calls，通过工具的调用+Tool Calls 才能够改变环境的状态，继续观察，才能够进一步的思考<br><img src="/images/article/ReAct+Tool.png" alt="图片"></p>
<center><b>ReAct + Tool Calls</b></center>
当然，每种Agent认知架构都有自己独特的优势，至于选择哪一种，如何组合起来更实用，取决于具体需求、应用场景和期望的用户体验。选择适合应用的认知架构是大语言模型应用开发的一个关键步骤。

<p>好吧，今天的干货分享就到这里。我试图用比较简短的篇幅，从理论到实践，全面系统地分析了Agent技术的发展现状，希望能够为你的Agent应用开发提供了参考和启发。未来，Agent技术的进一步发展将深刻影响人工智能在各领域的应用，推动人机协同迈上新台阶。</p>
<h2 id="RAG增强检索"><a href="#RAG增强检索" class="headerlink" title="RAG增强检索"></a>RAG增强检索</h2><p><img src="/images/article/RAG.png" alt="图片"></p>
<h3 id="GraphRAG"><a href="#GraphRAG" class="headerlink" title="GraphRAG"></a>GraphRAG</h3><p>成本较高，图谱构建困难，计算超级复杂，新增数据繁琐<br>基于图的检索增强生成 (RAG) 方法，可以对私有或以前未见过的数据集进行问答。通过 LLM 构建知识图谱结合图机器学习，GraphRAG 极大增强 LLM 在处理私有数据时的性能，同时具备连点成线的跨大型数据集的复杂语义问题推理能力，其基于前置的知识图谱、社区分层和语义总结以及图机器学习技术可以大幅度提供此类场景的性能。<br>GraphRAG 方法可以归结为：利用大型语言模型 (LLMs) 从数据来源中提取知识图谱；将此图谱聚类成不同粒度级别的相关实体社区；对于 RAG 操作，遍历所有社区以创建“社区答案”，并进行缩减以创建最终答案。<br><img src="/images/article/LLM-zhilitu.jpeg" alt="图片"><br>GraphRAG的核心就是两个图，一个是文档图谱，一个是文档内部的实体关系图谱。</p>
<ul>
<li>文档（Document）表示系统输入的文档。这些可以代表CSV中的单独行或单独的.txt文件；</li>
<li>文本单元（TextUnit）表示待分析的文本块。这些块的大小、重叠以及是否遵守任何数据边界可以进行配置。</li>
<li>实体（Entity）表示从文本单元中提取的实体。这些代表人、地点、事件或您提供的其他实体模型；</li>
<li>关系（Relationship）表示两个实体之间的关系；</li>
<li>协变量（Covariate）表示提取的声明信息，其中包含可能有时间限制的关于实体的陈述；</li>
<li>社区报告（Community Report）表示一旦生成实体，会对它们执行层次化的社区检测，并为这个层次结构中的每个社区生成报告；</li>
<li>节点（Node）：包含已嵌入和聚类的实体和文档的渲染图视图的布局信息。</li>
</ul>
<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><h3 id="全量微调"><a href="#全量微调" class="headerlink" title="全量微调"></a>全量微调</h3><p>全量微调是指对整个预训练模型进行微调，包括所有的模型参数。在这种方法中，预训练模型的所有层和参数都会被更新和优化，以适应目标任务的需求。这种微调方法通常适用于任务和预训练模型之间存在较大差异的情况，或者任务需要模型具有高度灵活性和自适应能力的情况。Full Fine-tuning需要较大的计算资源和时间，但可以获得更好的性能。</p>
<h3 id="增量微调"><a href="#增量微调" class="headerlink" title="增量微调"></a>增量微调</h3><p>增量微调是指在微调过程中只更新模型的顶层或少数几层，而保持预训练模型的底层参数不变。这种方法的目的是在保留预训练模型的通用知识的同时，通过微调顶层来适应特定任务。Repurposing通常适用于目标任务与预训练模型之间有一定相似性的情况，或者任务数据集较小的情况。由于只更新少数层，Repurposing相对于Full Fine-tuning需要较少的计算资源和时间，但在某些情况下性能可能会有所降低。</p>
<h3 id="微调方法详解"><a href="#微调方法详解" class="headerlink" title="微调方法详解"></a>微调方法详解</h3><ol>
<li>Adapter调整<br>Adapter调整是在预训练模型的每个层或选定层之间插入小型神经网络模块（适配器）。这些适配器是可训练的，而原始模型的参数则保持不变。在微调过程中，只更新适配器的参数，从而实现对新任务的适应。</li>
<li>前缀调整<br>前缀调整是在输入序列前添加可训练、任务特定的前缀向量。这些前缀向量在训练过程中更新，以指导模型输出更适合特定任务的响应。前缀调整的优势在于不需要调整模型的所有权重，而是通过调整输入序列来影响模型输出。</li>
<li>低秩适应（LoRA）<br>LoRA方法通过引入两个低秩矩阵A和B来近似原始权重矩阵的更新。这两个低秩矩阵的维度远小于原始权重矩阵，从而减少了需要训练的参数数量。在微调过程中，只更新这两个低秩矩阵的参数，并将它们叠加到原始权重矩阵上，以实现模型行为的微调。<br>实施步骤</li>
<li>选择预训练模型<br>选择一个在大规模数据集上预训练好的模型，如BERT、GPT等。</li>
<li>准备新任务数据集<br>收集并处理与特定任务相关的数据集，包括训练集、验证集和测试集。</li>
<li>设置微调参数<br>根据任务特性和模型特点，设置合适的微调参数，如学习率、批处理大小、训练轮次等。</li>
<li>进行微调训练<br>在新任务数据集上对预训练模型进行进一步训练，通过调整模型权重和参数来优化模型在新任务上的性能。</li>
<li>评估与调优<br>使用验证集对微调后的模型进行评估，并根据评估结果调整模型结构和参数，直到达到满意的性能。</li>
<li>模型部署<br>将微调后的模型部署到实际的应用场景中，以实现模型的实用价值。</li>
</ol>
<h2 id="微调和RAG场景选择"><a href="#微调和RAG场景选择" class="headerlink" title="微调和RAG场景选择"></a>微调和RAG场景选择</h2><h3 id="场景维度"><a href="#场景维度" class="headerlink" title="场景维度"></a>场景维度</h3><ul>
<li>动态数据  RAG</li>
<li>模型能力定制  微调</li>
<li>幻觉  RAG&gt;微调</li>
<li>可解释性 RAG</li>
<li>成本  RAG</li>
<li>依赖通用能力  RAG</li>
<li>延迟  微调</li>
<li>智能设备 微调</li>
</ul>
<h3 id="场景案例"><a href="#场景案例" class="headerlink" title="场景案例"></a>场景案例</h3><ul>
<li>AI产品经理<ul>
<li>处理动态数据 RAG</li>
<li>超级对话能力 RAG</li>
<li>业务能力 RAG</li>
</ul>
</li>
<li>AI程序员<ul>
<li>很强规范阅读能力 微调</li>
<li>代码编写分析能力 微调</li>
</ul>
</li>
</ul>
</div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><span class="mr-2"><i class="fas fa-tags has-text-grey"></i></span><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Inference-Optimization/">Inference Optimization</a><a class="link-muted mr-2" rel="tag" href="/tags/Transformer/">Transformer</a><a class="link-muted mr-2" rel="tag" href="/tags/GPT/">GPT</a><a class="link-muted mr-2" rel="tag" href="/tags/OpenAI/">OpenAI</a><a class="link-muted mr-2" rel="tag" href="/tags/ChatGPT/">ChatGPT</a><a class="link-muted mr-2" rel="tag" href="/tags/ACCELERATED-COMPUTING/">ACCELERATED COMPUTING</a></div></div></article></div><div class="card"><article class="card-content article" role="article"><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/essays/%E8%AE%BA%E7%9F%A5%E8%B6%B3%E5%B8%B8%E4%B9%90/">论知足常乐</a></p><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i>2022-10-16</span><span class="level-item"><i class="far fa-calendar-check"> </i>2024-10-17</span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i><a class="link-muted" href="/essays/">essays</a></span><span class="level-item"><i class="far fa-clock"></i>a few seconds read (About 0 words)</span></div></div><div class="content"></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Shikang Pang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Shikang Pang</p><p class="is-block" style="white-space: pre; font-style: italic;margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Deep Learning 
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Wuhan, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/shikangpang" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/shikangpang"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/articles/"><span class="level-start"><span class="level-item">articles</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/essays/"><span class="level-start"><span class="level-item">essays</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/life/"><span class="level-start"><span class="level-item">life</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/readings/"><span class="level-start"><span class="level-item">readings</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/study/"><span class="level-start"><span class="level-item">study</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-16T12:16:37.000Z">2024-10-16</time></p><p class="title"><a href="/blog/Sobel%20Operator/">Sobel Operator</a></p><p class="categories"><a href="/blog/">blog</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-16T12:16:37.000Z">2024-10-16</time></p><p class="title"><a href="/life/life/">life</a></p><p class="categories"><a href="/life/">life</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-16T07:39:07.000Z">2024-10-16</time></p><p class="title"><a href="/detection/">detection</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-16T05:59:30.000Z">2024-10-16</time></p><p class="title"><a href="/readings/%E7%99%BE%E5%B9%B4%E5%AD%A4%E7%8B%AC/">百年孤独</a></p><p class="categories"><a href="/readings/">readings</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-16T05:59:30.000Z">2024-10-16</time></p><p class="title"><a href="/study/normal_psy/">普通心理学</a></p><p class="categories"><a href="/study/">study</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/ACCELERATED-COMPUTING/"><span class="tag">ACCELERATED COMPUTING</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChatGPT/"><span class="tag">ChatGPT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GPT/"><span class="tag">GPT</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Inference-Optimization/"><span class="tag">Inference Optimization</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenAI/"><span class="tag">OpenAI</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">2</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="SkPang" height="28"></a><p class="is-size-7"><span>&copy; 2024 Shikang Pang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2024</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><form class="searchbox-input-container"><input class="searchbox-input" name="wd" type="text" placeholder="Type something..."></form><a class="searchbox-close" href="javascript:;">×</a></div></div></div><script>(function ($) {
            $('.searchbox-input-container').on('submit', function (e) {
                var keyword = $('.searchbox-input[name="wd"]').val();
                window.location = 'https://www.baidu.com/s?wd=site:shikangpang.github.io ' + keyword;
                return false;
            });
        })(jQuery);
        (function (document, $) {
            $(document).on('click', '.navbar-main .search', function () {
                $('.searchbox').toggleClass('show');
            }).on('click', '.searchbox .searchbox-mask', function () {
                $('.searchbox').removeClass('show');
            }).on('click', '.searchbox-close', function () {
                $('.searchbox').removeClass('show');
            });
        })(document, jQuery);</script></body></html>